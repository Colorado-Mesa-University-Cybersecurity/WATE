{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device_in_use = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Averaging Ensembling Vs Regular Training\n",
    "### Same number of epochs, same dataset and split\n",
    "**Weight averaging clearly results in a lower expected loss and variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14447\n",
      "3096\n",
      "3097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "#reading in dataset\n",
    "ch = fetch_california_housing()\n",
    "df = pd.DataFrame(data=ch.data, columns=ch.feature_names)\n",
    "\n",
    "df_train, df_temp = train_test_split(df, train_size=.70, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=.5, random_state=42)\n",
    "\n",
    "print(df_train.shape[0])\n",
    "print(df_val.shape[0])\n",
    "print(df_test.shape[0])\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = dataframe.drop('MedInc', axis=1).values\n",
    "        self.labels = dataframe['MedInc'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "train_dataset = CustomDataset(df_train)\n",
    "val_dataset = CustomDataset(df_val)\n",
    "test_dataset = CustomDataset(df_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to move an entire dataset to the device in advance\n",
    "def preload_dataset_to_device(loader, batch_size, device):\n",
    "    preloaded_data = [(inputs.to(device), targets.to(device)) for inputs, targets in loader]\n",
    "    return DataLoader(preloaded_data, batch_size=batch_size)\n",
    "\n",
    "# Preload datasets to device (if they fit into your device memory)\n",
    "train_loader = preload_dataset_to_device(train_loader, len(train_dataset), device_in_use)\n",
    "val_loader = preload_dataset_to_device(val_loader, len(val_dataset), device_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "# Example parameters\n",
    "input_size = 7  # Number of input features\n",
    "output_size = 1  # Number of output features (for regression tasks)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0000002)  # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def harness(epochs, train_loader, val_loader):\n",
    "    # Initialize the model\n",
    "    model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  \n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs)  \n",
    "            loss = criterion(outputs, targets.unsqueeze(2))  \n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "        train_loss = loss\n",
    "\n",
    "\n",
    "        model.eval()  \n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.unsqueeze(2))\n",
    "\n",
    "        test_loss = loss\n",
    "\n",
    "    return test_loss.item(), train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441.7445068359375\n",
      "444.7723083496094\n",
      "264.6326599121094\n",
      "266.943359375\n",
      "151.4241943359375\n",
      "128.75701904296875\n",
      "482.56494140625\n",
      "485.1209716796875\n",
      "167.10023498535156\n",
      "148.8119354248047\n",
      "32.21702575683594\n",
      "23.027896881103516\n",
      "327.7943115234375\n",
      "315.4488220214844\n",
      "31.97795295715332\n",
      "22.892133712768555\n",
      "10.379721641540527\n",
      "6.296657562255859\n",
      "219.43775939941406\n",
      "216.83673095703125\n",
      "1382.3360595703125\n",
      "1368.7249755859375\n",
      "521.5062866210938\n",
      "524.0630493164062\n",
      "51.77092361450195\n",
      "49.194374084472656\n",
      "62.38676834106445\n",
      "62.911380767822266\n",
      "253.6999053955078\n",
      "245.31967163085938\n",
      "30.61112403869629\n",
      "25.487802505493164\n",
      "35.26033401489258\n",
      "24.902217864990234\n",
      "17.64187240600586\n",
      "16.89542579650879\n",
      "966.4056396484375\n",
      "981.4949951171875\n",
      "28.91860008239746\n",
      "25.77888298034668\n",
      "511.3040466308594\n",
      "512.1527709960938\n",
      "38.1777458190918\n",
      "26.499223709106445\n",
      "330.36865234375\n",
      "319.0923156738281\n",
      "663.5975952148438\n",
      "661.4681396484375\n",
      "252.0320587158203\n",
      "249.5060577392578\n",
      "167.97650146484375\n",
      "163.39614868164062\n",
      "18.22220230102539\n",
      "12.340455055236816\n",
      "672.3510131835938\n",
      "657.2251586914062\n",
      "41.71381759643555\n",
      "33.30491638183594\n",
      "267.972412109375\n",
      "266.1603698730469\n",
      "24.54676628112793\n",
      "21.164451599121094\n",
      "317.1988525390625\n",
      "312.395263671875\n",
      "599.2077026367188\n",
      "599.9042358398438\n",
      "278.41363525390625\n",
      "282.6001281738281\n",
      "128.15740966796875\n",
      "128.53506469726562\n",
      "205.53074645996094\n",
      "206.4512939453125\n",
      "189.048828125\n",
      "182.31288146972656\n",
      "9.584003448486328\n",
      "8.376980781555176\n",
      "663.156005859375\n",
      "667.0498657226562\n",
      "47.61380386352539\n",
      "38.2747802734375\n",
      "60.672969818115234\n",
      "60.59885787963867\n",
      "39.8980827331543\n",
      "22.675716400146484\n",
      "605.6055908203125\n",
      "606.5200805664062\n",
      "134.64450073242188\n",
      "135.39874267578125\n",
      "48.87788391113281\n",
      "47.61449432373047\n",
      "202.83804321289062\n",
      "203.079345703125\n",
      "33.024898529052734\n",
      "21.415328979492188\n",
      "297.48126220703125\n",
      "295.6941223144531\n",
      "787.7996826171875\n",
      "795.6736450195312\n",
      "18.736989974975586\n",
      "17.543466567993164\n",
      "19.30824089050293\n",
      "18.698524475097656\n",
      "539.5285034179688\n",
      "544.0144653320312\n",
      "93.17461395263672\n",
      "72.64672088623047\n",
      "64.0838623046875\n",
      "50.97969436645508\n",
      "293.8216857910156\n",
      "285.04833984375\n",
      "86.41802978515625\n",
      "86.46898651123047\n",
      "190.36326599121094\n",
      "190.6495361328125\n",
      "184.79653930664062\n",
      "188.7137908935547\n",
      "96.2460708618164\n",
      "86.64736938476562\n",
      "462.641845703125\n",
      "453.2872314453125\n",
      "45.0821418762207\n",
      "44.704185485839844\n",
      "547.4752807617188\n",
      "539.8491821289062\n",
      "318.5922546386719\n",
      "313.2452697753906\n",
      "113.22325897216797\n",
      "112.47969818115234\n",
      "518.6226196289062\n",
      "520.7631225585938\n",
      "96.1961441040039\n",
      "95.69308471679688\n",
      "302.84515380859375\n",
      "297.47381591796875\n",
      "33.078895568847656\n",
      "33.503116607666016\n",
      "801.026611328125\n",
      "812.9383544921875\n",
      "174.4853515625\n",
      "170.81549072265625\n",
      "20.495386123657227\n",
      "7.012285232543945\n",
      "606.4609985351562\n",
      "613.3705444335938\n",
      "738.425537109375\n",
      "739.7234497070312\n",
      "33.91691589355469\n",
      "14.030460357666016\n",
      "32.46017074584961\n",
      "25.91716194152832\n",
      "456.7275695800781\n",
      "452.6748352050781\n",
      "43.96830368041992\n",
      "41.825321197509766\n",
      "11.714963912963867\n",
      "11.06810188293457\n",
      "653.3330688476562\n",
      "654.1256103515625\n",
      "689.56689453125\n",
      "691.7854614257812\n",
      "33.833396911621094\n",
      "34.1713981628418\n",
      "22.569469451904297\n",
      "20.07147216796875\n",
      "43.83229446411133\n",
      "35.77532958984375\n",
      "41.801658630371094\n",
      "27.496170043945312\n",
      "21.511343002319336\n",
      "20.136474609375\n",
      "609.8992919921875\n",
      "588.9522094726562\n",
      "140.24781799316406\n",
      "140.8775634765625\n",
      "165.7957305908203\n",
      "162.61378479003906\n",
      "98.51458740234375\n",
      "99.61955261230469\n",
      "83.52801513671875\n",
      "76.0704116821289\n",
      "97.26949310302734\n",
      "78.62291717529297\n",
      "80.54026794433594\n",
      "76.44256591796875\n",
      "38.43770217895508\n",
      "21.568622589111328\n",
      "554.131103515625\n",
      "563.7921142578125\n",
      "336.65985107421875\n",
      "339.1473083496094\n",
      "44.159088134765625\n",
      "43.68383026123047\n",
      "32.055118560791016\n",
      "30.47415542602539\n",
      "128.01071166992188\n",
      "117.2369613647461\n",
      "20.250839233398438\n",
      "17.682708740234375\n",
      "121.14568328857422\n",
      "122.9693374633789\n",
      "241.17838170051576 237.2566301345825\n",
      "261.2118783389218 263.419204969352\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    criterion = nn.MSELoss()  \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0000002) \n",
    "\n",
    "    testloss, trainloss = harness(model, 50, train_loader, val_loader)\n",
    "\n",
    "    train_loss.append(trainloss) \n",
    "    test_loss.append(testloss)\n",
    "    print(trainloss)\n",
    "    print(testloss)\n",
    "\n",
    "print(np.mean(train_loss), np.mean(test_loss))\n",
    "print(np.std(train_loss), np.std(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "test1, train1 = harness(model, 10, train_loader, val_loader)\n",
    "\n",
    "# Initialize the model\n",
    "model2 = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "test1, train1 = harness(model2, 10, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2808,  0.1736,  0.2340,  0.0231, -0.0548, -0.2834,  0.2829]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1017], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0573,  0.0978,  0.0317,  0.0038,  0.1684, -0.2727, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0103], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1118,  0.1357,  0.1329,  0.0134,  0.0568, -0.2781,  0.1326]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0560], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "averaged_model = LinearModel(input_size, output_size)\n",
    "\n",
    "# Iterate through the parameters of both models\n",
    "for param1, param2, param_avg in zip(model.parameters(), model2.parameters(), averaged_model.parameters()):\n",
    "    # Average the weights and update the parameters of the averaged_model\n",
    "    param_avg.data.copy_((param1.data + param2.data) / 2)\n",
    "\n",
    "x, y = model.parameters()\n",
    "print(x)\n",
    "print(y)\n",
    "x, y = model2.parameters()\n",
    "print(x)\n",
    "print(y)\n",
    "x, y = averaged_model.parameters()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498.8960876464844 496.87158203125\n"
     ]
    }
   ],
   "source": [
    "def weight_avg(epochs, train_loader, val_loader):\n",
    "    epochs = epochs//3\n",
    "\n",
    "    model1 = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion1 = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer1 = optim.SGD(model1.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "    model2 = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion2 = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer2 = optim.SGD(model2.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "    model3 = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion3 = nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer3 = optim.SGD(model3.parameters(), lr=0.0000002)  # Stochastic Gradient Descent\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model1.train()  \n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "            optimizer1.zero_grad()  \n",
    "            outputs = model1(inputs)  \n",
    "            loss = criterion1(outputs, targets.unsqueeze(2))  \n",
    "            loss.backward() \n",
    "            optimizer1.step() \n",
    "\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model2.train()  \n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "            optimizer2.zero_grad()  \n",
    "            outputs = model2(inputs)  \n",
    "            loss = criterion2(outputs, targets.unsqueeze(2))  \n",
    "            loss.backward() \n",
    "            optimizer2.step()\n",
    "\n",
    "    for param1, param2, param_avg in zip(model1.parameters(), model2.parameters(), model3.parameters()):\n",
    "        # Average the weights and update the parameters of the averaged_model\n",
    "        param_avg.data.copy_((param1.data + param2.data) / 2)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model3.train()  \n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "            optimizer3.zero_grad()  \n",
    "            outputs = model3(inputs)  \n",
    "            loss = criterion3(outputs, targets.unsqueeze(2))  \n",
    "            loss.backward() \n",
    "            optimizer3.step()\n",
    "\n",
    "        train_loss = loss\n",
    "\n",
    "\n",
    "        model3.eval()  \n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "                outputs = model3(inputs)\n",
    "                loss = criterion(outputs, targets.unsqueeze(2))\n",
    "\n",
    "        test_loss = loss\n",
    "    \n",
    "    return test_loss.item(), train_loss.item()\n",
    "    \n",
    "\n",
    "testloss, trainloss = weight_avg(120, train_loader, val_loader)\n",
    "print(testloss, trainloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional: 184.30738068342208 186.52231584919582\n",
      "Weight Avg: 119.71605833745002 137.85051488397693\n"
     ]
    }
   ],
   "source": [
    "train_loss_traditional = []\n",
    "test_loss_traditional = []\n",
    "\n",
    "train_loss_new = []\n",
    "test_loss_new = []\n",
    "\n",
    "for _ in range(1000):\n",
    "\n",
    "    model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "    criterion = nn.MSELoss()  \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0000002) \n",
    "\n",
    "    testloss, trainloss = harness(120, train_loader, val_loader)\n",
    "\n",
    "    train_loss_traditional.append(trainloss) \n",
    "    test_loss_traditional.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg(120, train_loader, val_loader)\n",
    "    train_loss_new.append(trainloss) \n",
    "    test_loss_new.append(testloss)\n",
    "\n",
    "\n",
    "print(\"Traditional:\",np.mean(test_loss_traditional),np.std(test_loss_traditional))\n",
    "print(\"Weight Avg:\",np.mean(test_loss_new),np.std(test_loss_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
