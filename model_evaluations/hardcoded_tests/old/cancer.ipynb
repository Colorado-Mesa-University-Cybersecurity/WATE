{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_avg_trees import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "device_in_use = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ch = load_breast_cancer()\n",
    "df = pd.DataFrame(data=ch.data, columns=ch.feature_names)\n",
    "# Assuming `ch.target` is the target variable\n",
    "df['Target'] = ch.target\n",
    "\n",
    "# Splitting the dataset\n",
    "df_train, df_temp = train_test_split(df, train_size=0.70, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=0.5, random_state=42)\n",
    "\n",
    "# Separate the target variable\n",
    "y_train = df_train['Target'].values\n",
    "y_val = df_val['Target'].values\n",
    "y_test = df_test['Target'].values\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data (excluding the target variable) and transform it\n",
    "scaled_train_features = scaler.fit_transform(df_train.drop(columns=['Target']))\n",
    "\n",
    "# Transform the validation and test data (excluding the target variable)\n",
    "scaled_val_features = scaler.transform(df_val.drop(columns=['Target']))\n",
    "scaled_test_features = scaler.transform(df_test.drop(columns=['Target']))\n",
    "\n",
    "# Recombine scaled features with target variable\n",
    "df_scaled_train = pd.DataFrame(scaled_train_features, columns=df_train.columns[:-1])  # Excluding the target variable column\n",
    "df_scaled_train['Target'] = y_train\n",
    "\n",
    "df_scaled_val = pd.DataFrame(scaled_val_features, columns=df_val.columns[:-1])\n",
    "df_scaled_val['Target'] = y_val\n",
    "\n",
    "df_scaled_test = pd.DataFrame(scaled_test_features, columns=df_test.columns[:-1])\n",
    "df_scaled_test['Target'] = y_test\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = dataframe.drop('Target', axis=1).values\n",
    "        self.labels = dataframe['Target'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = CustomDataset(df_scaled_train)\n",
    "val_dataset = CustomDataset(df_scaled_val)\n",
    "test_dataset = CustomDataset(df_scaled_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to move an entire dataset to the device in advance\n",
    "def preload_dataset_to_device(loader, batch_size, device):\n",
    "    preloaded_data = [(inputs.to(device), targets.to(device)) for inputs, targets in loader]\n",
    "    return DataLoader(preloaded_data, batch_size=batch_size)\n",
    "\n",
    "# Preload datasets to device (if they fit into your device memory)\n",
    "train_loader = preload_dataset_to_device(train_loader, len(train_dataset), device_in_use)\n",
    "val_loader = preload_dataset_to_device(val_loader, len(val_dataset), device_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:47<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional: 0.1710547363758087 0.0034637396087530033\n",
      "Weight Avg 2 Base Models: 0.2943999773263931 0.005980698999644444\n",
      "Weight Avg 4 Base Models: 0.46070484310388565 0.008638467151526335\n",
      "Weight Avg 8 Base Models: 0.5933123034238815 0.007556758551685531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = scaled_train_features.shape[1]\n",
    "output_size = 2\n",
    "\n",
    "train_loss_traditional = []\n",
    "test_loss_traditional = []\n",
    "\n",
    "train_loss_2 = []\n",
    "test_loss_2 = []\n",
    "\n",
    "train_loss_4 = []\n",
    "test_loss_4 = []\n",
    "\n",
    "train_loss_8 = []\n",
    "test_loss_8 = []\n",
    "\n",
    "#105 is the lcm(3,7,15)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "\n",
    "    testloss, trainloss = regular_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_traditional.append(trainloss) \n",
    "    test_loss_traditional.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_2_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_2.append(trainloss) \n",
    "    test_loss_2.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_4_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_4.append(trainloss) \n",
    "    test_loss_4.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_8_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_8.append(trainloss) \n",
    "    test_loss_8.append(testloss)\n",
    "\n",
    "\n",
    "print(\"Traditional:\",np.mean(test_loss_traditional),np.std(test_loss_traditional))\n",
    "print(\"Weight Avg 2 Base Models:\",np.mean(test_loss_2),np.std(test_loss_2))\n",
    "print(\"Weight Avg 4 Base Models:\",np.mean(test_loss_4),np.std(test_loss_4))\n",
    "print(\"Weight Avg 8 Base Models:\",np.mean(test_loss_8),np.std(test_loss_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional: 0.7533269420266151 0.2244393126898861\n",
      "Weight Avg 2 Base Models: 0.7639818024635315 0.1907219393882405\n",
      "Weight Avg 4 Base Models: 0.702273827791214 0.12840718709104437\n",
      "Weight Avg 8 Base Models: 0.6977672284841537 0.07924951804093364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = scaled_train_features.shape[1]\n",
    "output_size = 2\n",
    "\n",
    "train_loss_traditional = []\n",
    "test_loss_traditional = []\n",
    "\n",
    "train_loss_2 = []\n",
    "test_loss_2 = []\n",
    "\n",
    "train_loss_4 = []\n",
    "test_loss_4 = []\n",
    "\n",
    "train_loss_8 = []\n",
    "test_loss_8 = []\n",
    "\n",
    "#105 is the lcm(3,7,15)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "\n",
    "    testloss, trainloss = regular_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'Linear') \n",
    "\n",
    "    train_loss_traditional.append(trainloss) \n",
    "    test_loss_traditional.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_2_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'Linear') \n",
    "\n",
    "    train_loss_2.append(trainloss) \n",
    "    test_loss_2.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_4_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'Linear') \n",
    "\n",
    "    train_loss_4.append(trainloss) \n",
    "    test_loss_4.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_8_classification(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'Linear') \n",
    "\n",
    "    train_loss_8.append(trainloss) \n",
    "    test_loss_8.append(testloss)\n",
    "\n",
    "\n",
    "print(\"Traditional:\",np.mean(test_loss_traditional),np.std(test_loss_traditional))\n",
    "print(\"Weight Avg 2 Base Models:\",np.mean(test_loss_2),np.std(test_loss_2))\n",
    "print(\"Weight Avg 4 Base Models:\",np.mean(test_loss_4),np.std(test_loss_4))\n",
    "print(\"Weight Avg 8 Base Models:\",np.mean(test_loss_8),np.std(test_loss_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.7016337513923645 Test 0.6464226245880127\n",
      "Train 0.6630672812461853 Test 0.6075065732002258\n",
      "Train 0.6288616061210632 Test 0.573322057723999\n",
      "Train 0.5985026359558105 Test 0.5432292819023132\n",
      "Train 0.5714974403381348 Test 0.516645073890686\n",
      "Train 0.5473955273628235 Test 0.49305787682533264\n",
      "Train 0.5257981419563293 Test 0.47202885150909424\n",
      "Train 0.5063608288764954 Test 0.4531875550746918\n",
      "Train 0.4887898564338684 Test 0.43622323870658875\n",
      "Train 0.47283613681793213 Test 0.4208759069442749\n",
      "Train 0.45828938484191895 Test 0.40692803263664246\n",
      "Train 0.4449717700481415 Test 0.3941969573497772\n",
      "Train 0.4327324330806732 Test 0.3825294077396393\n",
      "Train 0.42144352197647095 Test 0.37179553508758545\n",
      "Train 0.4109960198402405 Test 0.3618853986263275\n",
      "Train 0.40129661560058594 Test 0.3527051508426666\n",
      "Train 0.3922649025917053 Test 0.34417474269866943\n",
      "Train 0.38383159041404724 Test 0.33622488379478455\n",
      "Train 0.3759368062019348 Test 0.3287960886955261\n",
      "Train 0.3685280978679657 Test 0.32183653116226196\n",
      "Train 0.3615599274635315 Test 0.3153010308742523\n",
      "Train 0.35499200224876404 Test 0.30915021896362305\n",
      "Train 0.34878891706466675 Test 0.30334940552711487\n",
      "Train 0.3429194986820221 Test 0.2978677749633789\n",
      "Train 0.33735600113868713 Test 0.2926784157752991\n",
      "Train 0.3320736885070801 Test 0.28775718808174133\n",
      "Train 0.32705044746398926 Test 0.28308263421058655\n",
      "Train 0.32226645946502686 Test 0.27863550186157227\n",
      "Train 0.3177039325237274 Test 0.27439871430397034\n",
      "Train 0.3133469521999359 Test 0.27035659551620483\n",
      "Train 0.3091808557510376 Test 0.26649534702301025\n",
      "Train 0.3051925599575043 Test 0.26280227303504944\n",
      "Train 0.30137014389038086 Test 0.25926584005355835\n",
      "Train 0.2977028489112854 Test 0.25587576627731323\n",
      "Train 0.2941805422306061 Test 0.2526225447654724\n",
      "Train 0.29079434275627136 Test 0.2494973987340927\n",
      "Train 0.2875359058380127 Test 0.24649251997470856\n",
      "Train 0.28439751267433167 Test 0.2436005026102066\n",
      "Train 0.28137221932411194 Test 0.24081481993198395\n",
      "Train 0.2784535586833954 Test 0.23812922835350037\n",
      "Train 0.27563562989234924 Test 0.235538050532341\n",
      "Train 0.27291277050971985 Test 0.2330361306667328\n",
      "Train 0.27027997374534607 Test 0.23061862587928772\n",
      "Train 0.26773250102996826 Test 0.22828097641468048\n",
      "Train 0.26526591181755066 Test 0.22601903975009918\n",
      "Train 0.26287612318992615 Test 0.22382892668247223\n",
      "Train 0.2605593800544739 Test 0.22170709073543549\n",
      "Train 0.2583119869232178 Test 0.21965013444423676\n",
      "Train 0.25613081455230713 Test 0.21765492856502533\n",
      "Train 0.2540125548839569 Test 0.21571852266788483\n",
      "Train 0.25195449590682983 Test 0.21383817493915558\n",
      "Train 0.2499537467956543 Test 0.21201136708259583\n",
      "Train 0.24800783395767212 Test 0.21023564040660858\n",
      "Train 0.2461143583059311 Test 0.20850877463817596\n",
      "Train 0.24427105486392975 Test 0.20682859420776367\n",
      "Train 0.242475688457489 Test 0.2051931768655777\n",
      "Train 0.24072648584842682 Test 0.20360051095485687\n",
      "Train 0.23902134597301483 Test 0.20204894244670868\n",
      "Train 0.2373586744070053 Test 0.20053675770759583\n",
      "Train 0.23573659360408783 Test 0.199062317609787\n",
      "Train 0.2341535985469818 Test 0.19762420654296875\n",
      "Train 0.23260819911956787 Test 0.1962210088968277\n",
      "Train 0.2310989648103714 Test 0.1948513239622116\n",
      "Train 0.22962455451488495 Test 0.1935139000415802\n",
      "Train 0.22818362712860107 Test 0.19220754504203796\n",
      "Train 0.22677496075630188 Test 0.19093111157417297\n",
      "Train 0.22539746761322021 Test 0.1896834820508957\n",
      "Train 0.22405003011226654 Test 0.1884637326002121\n",
      "Train 0.2227316051721573 Test 0.18727070093154907\n",
      "Train 0.22144106030464172 Test 0.18610365688800812\n",
      "Train 0.22017763555049896 Test 0.18496154248714447\n",
      "Train 0.21894033253192902 Test 0.18384358286857605\n",
      "Train 0.21772831678390503 Test 0.1827489733695984\n",
      "Train 0.21654073894023895 Test 0.18167690932750702\n",
      "Train 0.21537674963474274 Test 0.18062669038772583\n",
      "Train 0.2142357975244522 Test 0.17959755659103394\n",
      "Train 0.21311689913272858 Test 0.17858892679214478\n",
      "Train 0.21201947331428528 Test 0.17760001122951508\n",
      "Train 0.21094299852848053 Test 0.17663033306598663\n",
      "Train 0.20988662540912628 Test 0.17567922174930573\n",
      "Train 0.2088499218225479 Test 0.17474615573883057\n",
      "Train 0.20783215761184692 Test 0.1738305240869522\n",
      "Train 0.2068329006433487 Test 0.17293189465999603\n",
      "Train 0.20585152506828308 Test 0.17204973101615906\n",
      "Train 0.20488764345645905 Test 0.1711835265159607\n",
      "Train 0.20394058525562286 Test 0.17033283412456512\n",
      "Train 0.2030099779367447 Test 0.16949723660945892\n",
      "Train 0.2020953744649887 Test 0.16867634654045105\n",
      "Train 0.2011963278055191 Test 0.16786962747573853\n",
      "Train 0.20031240582466125 Test 0.16707676649093628\n",
      "Train 0.19944319128990173 Test 0.16629742085933685\n",
      "Train 0.1985882967710495 Test 0.16553112864494324\n",
      "Train 0.1977473348379135 Test 0.16477763652801514\n",
      "Train 0.1969199776649475 Test 0.16403654217720032\n",
      "Train 0.19610583782196045 Test 0.16330751776695251\n",
      "Train 0.19530461728572845 Test 0.16259030997753143\n",
      "Train 0.19451595842838287 Test 0.16188457608222961\n",
      "Train 0.19373954832553864 Test 0.16119003295898438\n",
      "Train 0.19297508895397186 Test 0.16050636768341064\n",
      "Train 0.19222228229045868 Test 0.15983331203460693\n",
      "Train 0.1914808601140976 Test 0.15917064249515533\n",
      "Train 0.19075050950050354 Test 0.15851809084415436\n",
      "Train 0.1900310069322586 Test 0.15787537395954132\n",
      "Train 0.1893220841884613 Test 0.1572422981262207\n",
      "Train 0.18862347304821014 Test 0.1566186100244522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model = LinearModel(input_size, output_size).to(device_in_use)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "for epoch in range(105):\n",
    "    model.train()  \n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs.squeeze(0), targets.squeeze(0))  \n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "\n",
    "    train_loss = loss\n",
    "\n",
    "\n",
    "    model.eval()  \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device_in_use), targets.to(device_in_use)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(0), targets.squeeze(0))\n",
    "\n",
    "    test_loss = loss\n",
    "    \n",
    "    print(\"Train\", train_loss.item(), \"Test\", test_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
