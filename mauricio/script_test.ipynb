{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import DeepTreeEnsemble\n",
    "import torch.nn.functional as F\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "reload(DeepTreeEnsemble)\n",
    "from DeepTreeEnsemble import DeepTreeEnsemble, create_dataloaders_for_dataset, create_imageloaders_for_dataset\n",
    "# if this ^ doesn't work, try 'pip install -e .' in the root directory of it. Make sure you are in \n",
    "# the desired environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, input_size, output_size = create_imageloaders_for_dataset('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, dropout=.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Create the first hidden layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_layers[0]))\n",
    "\n",
    "        # Create additional hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_layers[i]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        i = 0\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            # x = F.relu(batch_norm(layer(x)))\n",
    "            i=i+1\n",
    "            x = self.dropout(x)\n",
    "            # print('Layer dtype: ', layer.weight.dtype)\n",
    "            x = x.to(torch.float32) # <-- area of concern\n",
    "            # print('tensor dytpe: ', x.dtype)\n",
    "            # layer.weight = layer.weight.to(torch.float32)\n",
    "            x = F.relu(batch_norm(layer(x)))\n",
    "            \n",
    "            # print(f'dtype is {x.dtype} after layer {i}')\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got here\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [25, 25]\n",
    "multiLayerPerceptron = MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size+1, dropout=.5) #hidden size default at 100\n",
    "print(\"got here\")\n",
    "# Fill DTE params\n",
    "modelSaveDirectory = './saved_models/'\n",
    "DTEmlp4b = DeepTreeEnsemble(task_name='classification',\n",
    "                                model_arch=multiLayerPerceptron,\n",
    "                                model_dir=modelSaveDirectory, \n",
    "                                base_number=4,\n",
    "                                epochs=10,\n",
    "                                train_dataloader=train_dataloader, \n",
    "                                test_dataloader=test_dataloader,\n",
    "                                learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(DTE, trails, output_directory):\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for i in range(trails):\n",
    "        df1 = DTE.train_DTE()\n",
    "        df2 = DTE.single_model()\n",
    "        combined_df = pd.concat([df1, df2], axis=1)\n",
    "        results = pd.concat([results, combined_df], axis=0)\n",
    "\n",
    "    results.to_csv(output_directory, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initializing 4 models\n",
      "Training Model with id 1\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:45<00:00, 10.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 2.4064688682556152, Train loss end: 1.7962603569030762\n",
      "Highest Mean Train Accuracy (over epoch): 0.573724452014218\n",
      "Test loss start: 2.1672325134277344, Test loss end: 1.731744647026062\n",
      "Highest mean Test Accuracy (over epoch): 0.7231715425531915\n",
      " Test F1 score at highest accuracy: 0.530062355407572\n",
      "./saved_models/model_1.pth \n",
      "\n",
      "Training Model with id 2\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:41<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 2.401355743408203, Train loss end: 1.9344689846038818\n",
      "Highest Mean Train Accuracy (over epoch): 0.5830976007109004\n",
      "Test loss start: 2.122525215148926, Test loss end: 1.627313256263733\n",
      "Highest mean Test Accuracy (over epoch): 0.776529255319149\n",
      " Test F1 score at highest accuracy: 0.5639517594193757\n",
      "./saved_models/model_2.pth \n",
      "\n",
      "Training Model with id 3\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:44<00:00, 10.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 2.395130157470703, Train loss end: 1.8159599304199219\n",
      "Highest Mean Train Accuracy (over epoch): 0.6038396030805687\n",
      "Test loss start: 2.198685646057129, Test loss end: 1.6695746183395386\n",
      "Highest mean Test Accuracy (over epoch): 0.8142619680851064\n",
      " Test F1 score at highest accuracy: 0.5953953037341628\n",
      "./saved_models/model_3.pth \n",
      "\n",
      "Training Model with id 4\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:39<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 2.3999381065368652, Train loss end: 1.9246865510940552\n",
      "Highest Mean Train Accuracy (over epoch): 0.5810000740521327\n",
      "Test loss start: 2.2363533973693848, Test loss end: 1.6647608280181885\n",
      "Highest mean Test Accuracy (over epoch): 0.7616356382978723\n",
      " Test F1 score at highest accuracy: 0.5526855358813791\n",
      "./saved_models/model_4.pth \n",
      "\n",
      "Training Model with id 7\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:41<00:00, 10.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 1.8689384460449219, Train loss end: 1.935454249382019\n",
      "Highest Mean Train Accuracy (over epoch): 0.709378702606635\n",
      "Test loss start: 1.7186944484710693, Test loss end: 1.5569318532943726\n",
      "Highest mean Test Accuracy (over epoch): 0.874185505319149\n",
      " Test F1 score at highest accuracy: 0.7096139683532475\n",
      "./saved_models/model_7.pth \n",
      "\n",
      "Training Model with id 8\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:39<00:00,  9.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 1.8341199159622192, Train loss end: 1.9060267210006714\n",
      "Highest Mean Train Accuracy (over epoch): 0.7354765254739336\n",
      "Test loss start: 1.7119882106781006, Test loss end: 1.5504266023635864\n",
      "Highest mean Test Accuracy (over epoch): 0.8930684840425532\n",
      " Test F1 score at highest accuracy: 0.7373557868357229\n",
      "./saved_models/model_8.pth \n",
      "\n",
      "Training Model with id 6\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process: 100%|██████████| 10/10 [01:39<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epochs: 10\n",
      "Train loss start: 1.767219066619873, Train loss end: 1.7720463275909424\n",
      "Highest Mean Train Accuracy (over epoch): 0.74700273992891\n",
      "Test loss start: 1.6875070333480835, Test loss end: 1.551195740699768\n",
      "Highest mean Test Accuracy (over epoch): 0.8966921542553191\n",
      " Test F1 score at highest accuracy: 0.7487135684586669\n",
      "./saved_models/model_6.pth \n",
      "\n",
      " Maximum accuracy: 0.8966921542553191, F1: 0.7487135684586669, Test: 1.6508420420453904, Train: 1.8018218632911054\n",
      "in multi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Process:  79%|███████▊  | 55/70 [11:50<03:13, 12.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDTEmlp4b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(DTE, trails, output_directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trails):\n\u001b[1;32m      5\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m DTE\u001b[38;5;241m.\u001b[39mtrain_DTE()\n\u001b[0;32m----> 6\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m \u001b[43mDTE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, combined_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/ML-Work/WATE/mauricio/DeepTreeEnsemble.py:711\u001b[0m, in \u001b[0;36mDeepTreeEnsemble.single_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# Backward and optimizeytpe:  torch.float32Layer dtype:  torch.float32tensor dytpe:  torch.float32Layer dtype:  torch.float32tensor dytpe:  torch.int64\u001b[39;00m\n\u001b[1;32m    710\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 711\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Track the accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_directory = './output.csv'\n",
    "run_experiment(DTEmlp4b, trails=5, output_directory=output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [25, 25]\n",
    "multiLayerPerceptron = MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size+1, dropout=.5) #hidden size default at 100\n",
    "\n",
    "# Fill DTE params\n",
    "modelSaveDirectory = './saved_models/'\n",
    "DTEmlp8b = DeepTreeEnsemble(task_name='classification',\n",
    "                                model_arch=multiLayerPerceptron,\n",
    "                                model_dir=modelSaveDirectory, \n",
    "                                base_number=4,\n",
    "                                epochs=10,\n",
    "                                train_dataloader=train_dataloader, \n",
    "                                test_dataloader=test_dataloader,\n",
    "                                learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DTEmlp8b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputBase8.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m run_experiment(\u001b[43mDTEmlp8b\u001b[49m, trails\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_directory\u001b[38;5;241m=\u001b[39moutput_directory)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DTEmlp8b' is not defined"
     ]
    }
   ],
   "source": [
    "output_directory = './outputBase8.csv'\n",
    "run_experiment(DTEmlp8b, trails=5, output_directory=output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
