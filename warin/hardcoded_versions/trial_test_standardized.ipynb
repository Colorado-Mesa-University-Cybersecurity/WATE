{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_avg_trees import LinearModel, weight_avg_2, weight_avg_4, weight_avg_8, regular\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device_in_use = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the data\n",
    "ch = fetch_california_housing()\n",
    "df = pd.DataFrame(data=ch.data, columns=ch.feature_names)\n",
    "# Assuming `ch.target` is the target variable\n",
    "df['Target'] = ch.target\n",
    "\n",
    "# Splitting the dataset\n",
    "df_train, df_temp = train_test_split(df, train_size=0.70, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=0.5, random_state=42)\n",
    "\n",
    "# Separate the target variable\n",
    "y_train = df_train['Target'].values\n",
    "y_val = df_val['Target'].values\n",
    "y_test = df_test['Target'].values\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data (excluding the target variable) and transform it\n",
    "scaled_train_features = scaler.fit_transform(df_train.drop(columns=['Target']))\n",
    "\n",
    "# Transform the validation and test data (excluding the target variable)\n",
    "scaled_val_features = scaler.transform(df_val.drop(columns=['Target']))\n",
    "scaled_test_features = scaler.transform(df_test.drop(columns=['Target']))\n",
    "\n",
    "# Recombine scaled features with target variable\n",
    "df_scaled_train = pd.DataFrame(scaled_train_features, columns=df_train.columns[:-1])  # Excluding the target variable column\n",
    "df_scaled_train['Target'] = y_train\n",
    "\n",
    "df_scaled_val = pd.DataFrame(scaled_val_features, columns=df_val.columns[:-1])\n",
    "df_scaled_val['Target'] = y_val\n",
    "\n",
    "df_scaled_test = pd.DataFrame(scaled_test_features, columns=df_test.columns[:-1])\n",
    "df_scaled_test['Target'] = y_test\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = dataframe.drop('Target', axis=1).values\n",
    "        self.labels = dataframe['Target'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "train_dataset = CustomDataset(df_scaled_train)\n",
    "val_dataset = CustomDataset(df_scaled_val)\n",
    "test_dataset = CustomDataset(df_scaled_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to move an entire dataset to the device in advance\n",
    "def preload_dataset_to_device(loader, batch_size, device):\n",
    "    preloaded_data = [(inputs.to(device), targets.to(device)) for inputs, targets in loader]\n",
    "    return DataLoader(preloaded_data, batch_size=batch_size)\n",
    "\n",
    "# Preload datasets to device (if they fit into your device memory)\n",
    "train_loader = preload_dataset_to_device(train_loader, len(train_dataset), device_in_use)\n",
    "val_loader = preload_dataset_to_device(val_loader, len(val_dataset), device_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 7658/20000 [3:30:08<5:38:40,  1.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20000\u001b[39m)):\n\u001b[1;32m---> 21\u001b[0m     testloss, trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mregular\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m105\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_in_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     23\u001b[0m     train_loss_traditional\u001b[38;5;241m.\u001b[39mappend(trainloss) \n\u001b[0;32m     24\u001b[0m     test_loss_traditional\u001b[38;5;241m.\u001b[39mappend(testloss)\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\projects\\WATE\\warin\\hardcoded_versions\\weight_avg_trees.py:649\u001b[0m, in \u001b[0;36mregular\u001b[1;34m(epochs, train_loader, val_loader, input_size, output_size, device_in_use, model)\u001b[0m\n\u001b[0;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \n\u001b[0;32m    648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))  \n\u001b[1;32m--> 649\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    650\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[0;32m    652\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\smbm2\\AppData\\Local\\miniconda3\\envs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_size = scaled_train_features.shape[1]\n",
    "output_size = 1  \n",
    "\n",
    "train_loss_traditional = []\n",
    "test_loss_traditional = []\n",
    "\n",
    "train_loss_2 = []\n",
    "test_loss_2 = []\n",
    "\n",
    "train_loss_4 = []\n",
    "test_loss_4 = []\n",
    "\n",
    "train_loss_8 = []\n",
    "test_loss_8 = []\n",
    "\n",
    "#105 is the lcm(3,7,15)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(20000)):\n",
    "\n",
    "    testloss, trainloss = regular(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_traditional.append(trainloss) \n",
    "    test_loss_traditional.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_2(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_2.append(trainloss) \n",
    "    test_loss_2.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_4(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_4.append(trainloss) \n",
    "    test_loss_4.append(testloss)\n",
    "\n",
    "    testloss, trainloss = weight_avg_8(105, train_loader, val_loader, input_size, output_size, device_in_use, model = 'NN') \n",
    "\n",
    "    train_loss_8.append(trainloss) \n",
    "    test_loss_8.append(testloss)\n",
    "\n",
    "\n",
    "print(\"Traditional:\",np.mean(test_loss_traditional),np.std(test_loss_traditional))\n",
    "print(\"Weight Avg 2 Base Models:\",np.mean(test_loss_2),np.std(train_loss_2))\n",
    "print(\"Weight Avg 4 Base Models:\",np.mean(test_loss_4),np.std(train_loss_4))\n",
    "print(\"Weight Avg 8 Base Models:\",np.mean(test_loss_8),np.std(train_loss_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional: 0.9670778355887366 0.052957925827381534\n",
      "Weight Avg 2 Base Models: 0.9496065442012289 0.02592568634881894\n",
      "Weight Avg 4 Base Models: 0.9437944365251342 0.012429520639051917\n"
     ]
    }
   ],
   "source": [
    "print(\"Traditional:\",np.mean(test_loss_traditional),np.std(test_loss_traditional))\n",
    "print(\"Weight Avg 2 Base Models:\",np.mean(test_loss_2),np.std(train_loss_2))\n",
    "print(\"Weight Avg 4 Base Models:\",np.mean(test_loss_4),np.std(train_loss_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7658"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loss_traditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
